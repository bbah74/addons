{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tensorflow.compat.v1.disable_eager_execution()\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.keras import activations\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras import constraints\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.engine.base_layer import Layer\n",
    "from tensorflow.python.keras.engine.input_spec import InputSpec\n",
    "from tensorflow.python.keras.utils import generic_utils\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_util\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.training.tracking import base as trackable\n",
    "from tensorflow.python.training.tracking import data_structures\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECNN classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers.recurrent import RNN,DropoutRNNCellMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECNNCell class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECNNCell(DropoutRNNCellMixin, Layer):\n",
    "  \"\"\"Cell class for ECNN.\n",
    "\n",
    "  Arguments:\n",
    "    units: Positive integer, dimensionality of the output space.\n",
    "    activation: Activation function to use.\n",
    "      Default: hyperbolic tangent (`tanh`).\n",
    "      If you pass `None`, no activation is applied\n",
    "      (ie. \"linear\" activation: `a(x) = x`).\n",
    "    use_bias: Boolean, whether the layer uses a bias vector.\n",
    "    kernel_initializer: Initializer for the `kernel` weights matrix,\n",
    "      used for the linear transformation of the inputs.\n",
    "    recurrent_initializer: Initializer for the `recurrent_kernel`\n",
    "      weights matrix, used for the linear transformation of the recurrent state.\n",
    "    bias_initializer: Initializer for the bias vector.\n",
    "    kernel_regularizer: Regularizer function applied to\n",
    "      the `kernel` weights matrix.\n",
    "    recurrent_regularizer: Regularizer function applied to\n",
    "      the `recurrent_kernel` weights matrix.\n",
    "    bias_regularizer: Regularizer function applied to the bias vector.\n",
    "    kernel_constraint: Constraint function applied to\n",
    "      the `kernel` weights matrix.\n",
    "    recurrent_constraint: Constraint function applied to\n",
    "      the `recurrent_kernel` weights matrix.\n",
    "    bias_constraint: Constraint function applied to the bias vector.\n",
    "    dropout: Float between 0 and 1.\n",
    "      Fraction of the units to drop for\n",
    "      the linear transformation of the inputs.\n",
    "    recurrent_dropout: Float between 0 and 1.\n",
    "      Fraction of the units to drop for\n",
    "      the linear transformation of the recurrent state.\n",
    "\n",
    "  Call arguments:\n",
    "    inputs: A 2D tensor.\n",
    "    states: List of state tensors corresponding to the previous timestep.\n",
    "    training: Python boolean indicating whether the layer should behave in\n",
    "      training mode or in inference mode. Only relevant when `dropout` or\n",
    "      `recurrent_dropout` is used.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               units,\n",
    "               activation='tanh',\n",
    "               use_bias=True,\n",
    "               kernel_initializer='glorot_uniform',\n",
    "               recurrent_initializer='orthogonal',\n",
    "               bias_initializer='zeros',\n",
    "               kernel_regularizer=None,\n",
    "               recurrent_regularizer=None,\n",
    "               bias_regularizer=None,\n",
    "               kernel_constraint=None,\n",
    "               recurrent_constraint=None,\n",
    "               bias_constraint=None,\n",
    "               dropout=0.,\n",
    "               recurrent_dropout=0.,\n",
    "               **kwargs):\n",
    "    super(ECNNCell, self).__init__(**kwargs)\n",
    "    self.units = units\n",
    "    self.activation = activations.get(activation)\n",
    "    self.use_bias = use_bias\n",
    "\n",
    "    self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "    self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "    self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
    "    self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "    self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "    self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
    "    self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "    self.dropout = min(1., max(0., dropout))\n",
    "    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
    "    self.state_size = self.units\n",
    "    self.output_size = self.units\n",
    "\n",
    "  @tf_utils.shape_type_conversion\n",
    "  def build(self, input_shape):\n",
    "    self.kernel = self.add_weight(\n",
    "        shape=(input_shape[-1]-1, self.units),\n",
    "        name='kernel',\n",
    "        initializer=self.kernel_initializer,\n",
    "        regularizer=self.kernel_regularizer,\n",
    "        constraint=self.kernel_constraint)\n",
    "    \n",
    "    self.D = self.add_weight(shape=(1, self.units),\n",
    "                                      name='kernel',\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "    self.C = self.add_weight(shape=(self.units,1),\n",
    "                                      name='kernel',\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "    self.recurrent_kernel = self.add_weight(shape=(self.units, self.units),\n",
    "                                      name='recurrent_kernel',\n",
    "                                      initializer=self.recurrent_initializer,\n",
    "                                      regularizer=self.recurrent_regularizer,\n",
    "                                      constraint=self.recurrent_constraint)\n",
    "    if self.use_bias:\n",
    "      self.bias = self.add_weight(\n",
    "          shape=(self.units,),\n",
    "          name='bias',\n",
    "          initializer=self.bias_initializer,\n",
    "          regularizer=self.bias_regularizer,\n",
    "          constraint=self.bias_constraint)\n",
    "    else:\n",
    "      self.bias = None\n",
    "    self.built = True\n",
    "\n",
    " \n",
    "\n",
    "  def call(self, inputs, states, training=None):\n",
    "    prev_output = states[0]\n",
    "    dp_mask = self.get_dropout_mask_for_cell(inputs, training)\n",
    "    rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n",
    "        prev_output, training)\n",
    "    obs= inputs[:,-1]\n",
    "    obs = K.expand_dims(obs,axis = 1)\n",
    "    if dp_mask is not None:\n",
    "      h = K.dot(inputs[:,:-1]* dp_mask, self.kernel)\n",
    "    else:\n",
    "      h = K.dot(inputs[:,:-1], self.kernel)\n",
    "    if self.bias is not None:\n",
    "      h = K.bias_add(h, self.bias)\n",
    "\n",
    "    if rec_dp_mask is not None:\n",
    "      prev_output *= rec_dp_mask\n",
    "    simple_rec = h + K.dot(prev_output, self.recurrent_kernel)\n",
    "    Error = K.dot((K.tanh(K.dot(prev_output, self.C) - obs)),self.D)\n",
    "    output = simple_rec + Error\n",
    "    if self.activation is not None:\n",
    "      output = self.activation(output)\n",
    "\n",
    "    return output, [output]\n",
    "\n",
    "  #def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "  #  return _generate_zero_filled_state_for_cell(self, inputs, batch_size, dtype)\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {\n",
    "        'units':\n",
    "            self.units,\n",
    "        'activation':\n",
    "            activations.serialize(self.activation),\n",
    "        'use_bias':\n",
    "            self.use_bias,\n",
    "        'kernel_initializer':\n",
    "            initializers.serialize(self.kernel_initializer),\n",
    "        'recurrent_initializer':\n",
    "            initializers.serialize(self.recurrent_initializer),\n",
    "        'bias_initializer':\n",
    "            initializers.serialize(self.bias_initializer),\n",
    "        'kernel_regularizer':\n",
    "            regularizers.serialize(self.kernel_regularizer),\n",
    "        'recurrent_regularizer':\n",
    "            regularizers.serialize(self.recurrent_regularizer),\n",
    "        'bias_regularizer':\n",
    "            regularizers.serialize(self.bias_regularizer),\n",
    "        'kernel_constraint':\n",
    "            constraints.serialize(self.kernel_constraint),\n",
    "        'recurrent_constraint':\n",
    "            constraints.serialize(self.recurrent_constraint),\n",
    "        'bias_constraint':\n",
    "            constraints.serialize(self.bias_constraint),\n",
    "        'dropout':\n",
    "            self.dropout,\n",
    "        'recurrent_dropout':\n",
    "            self.recurrent_dropout\n",
    "    }\n",
    "    base_config = super(ECNNCell, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECNN(RNN):\n",
    "  \"\"\"Fully-connected RNN where the output is to be fed back to input.\n",
    "\n",
    "  Arguments:\n",
    "    units: Positive integer, dimensionality of the output space.\n",
    "    activation: Activation function to use.\n",
    "      Default: hyperbolic tangent (`tanh`).\n",
    "      If you pass None, no activation is applied\n",
    "      (ie. \"linear\" activation: `a(x) = x`).\n",
    "    use_bias: Boolean, whether the layer uses a bias vector.\n",
    "    kernel_initializer: Initializer for the `kernel` weights matrix,\n",
    "      used for the linear transformation of the inputs.\n",
    "    recurrent_initializer: Initializer for the `recurrent_kernel`\n",
    "      weights matrix,\n",
    "      used for the linear transformation of the recurrent state.\n",
    "    bias_initializer: Initializer for the bias vector.\n",
    "    kernel_regularizer: Regularizer function applied to\n",
    "      the `kernel` weights matrix.\n",
    "    recurrent_regularizer: Regularizer function applied to\n",
    "      the `recurrent_kernel` weights matrix.\n",
    "    bias_regularizer: Regularizer function applied to the bias vector.\n",
    "    activity_regularizer: Regularizer function applied to\n",
    "      the output of the layer (its \"activation\")..\n",
    "    kernel_constraint: Constraint function applied to\n",
    "      the `kernel` weights matrix.\n",
    "    recurrent_constraint: Constraint function applied to\n",
    "      the `recurrent_kernel` weights matrix.\n",
    "    bias_constraint: Constraint function applied to the bias vector.\n",
    "    dropout: Float between 0 and 1.\n",
    "      Fraction of the units to drop for\n",
    "      the linear transformation of the inputs.\n",
    "    recurrent_dropout: Float between 0 and 1.\n",
    "      Fraction of the units to drop for\n",
    "      the linear transformation of the recurrent state.\n",
    "    return_sequences: Boolean. Whether to return the last output\n",
    "      in the output sequence, or the full sequence.\n",
    "    return_state: Boolean. Whether to return the last state\n",
    "      in addition to the output.\n",
    "    go_backwards: Boolean (default False).\n",
    "      If True, process the input sequence backwards and return the\n",
    "      reversed sequence.\n",
    "    stateful: Boolean (default False). If True, the last state\n",
    "      for each sample at index i in a batch will be used as initial\n",
    "      state for the sample of index i in the following batch.\n",
    "    unroll: Boolean (default False).\n",
    "      If True, the network will be unrolled,\n",
    "      else a symbolic loop will be used.\n",
    "      Unrolling can speed-up a RNN,\n",
    "      although it tends to be more memory-intensive.\n",
    "      Unrolling is only suitable for short sequences.\n",
    "\n",
    "  Call arguments:\n",
    "    inputs: A 3D tensor.\n",
    "    mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n",
    "      a given timestep should be masked.\n",
    "    training: Python boolean indicating whether the layer should behave in\n",
    "      training mode or in inference mode. This argument is passed to the cell\n",
    "      when calling it. This is only relevant if `dropout` or\n",
    "      `recurrent_dropout` is used.\n",
    "    initial_state: List of initial state tensors to be passed to the first\n",
    "      call of the cell.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               units,\n",
    "               activation='tanh',\n",
    "               use_bias=True,\n",
    "               kernel_initializer='glorot_uniform',\n",
    "               recurrent_initializer='orthogonal',\n",
    "               bias_initializer='zeros',\n",
    "               kernel_regularizer=None,\n",
    "               recurrent_regularizer=None,\n",
    "               bias_regularizer=None,\n",
    "               activity_regularizer=None,\n",
    "               kernel_constraint=None,\n",
    "               recurrent_constraint=None,\n",
    "               bias_constraint=None,\n",
    "               dropout=0.,\n",
    "               recurrent_dropout=0.,\n",
    "               return_sequences=False,\n",
    "               return_state=False,\n",
    "               go_backwards=False,\n",
    "               stateful=False,\n",
    "               unroll=False,\n",
    "               **kwargs):\n",
    "    if 'implementation' in kwargs:\n",
    "      kwargs.pop('implementation')\n",
    "      logging.warning('The `implementation` argument '\n",
    "                      'in `SimpleRNN` has been deprecated. '\n",
    "                      'Please remove it from your layer call.')\n",
    "    cell = ECNNCell(\n",
    "        units,\n",
    "        activation=activation,\n",
    "        use_bias=use_bias,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        recurrent_initializer=recurrent_initializer,\n",
    "        bias_initializer=bias_initializer,\n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        recurrent_regularizer=recurrent_regularizer,\n",
    "        bias_regularizer=bias_regularizer,\n",
    "        kernel_constraint=kernel_constraint,\n",
    "        recurrent_constraint=recurrent_constraint,\n",
    "        bias_constraint=bias_constraint,\n",
    "        dropout=dropout,\n",
    "        recurrent_dropout=recurrent_dropout)\n",
    "    super(ECNN, self).__init__(\n",
    "        cell,\n",
    "        return_sequences=return_sequences,\n",
    "        return_state=return_state,\n",
    "        go_backwards=go_backwards,\n",
    "        stateful=stateful,\n",
    "        unroll=unroll,\n",
    "        **kwargs)\n",
    "    self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "    self.input_spec = [InputSpec(ndim=3)]\n",
    "\n",
    "  def call(self, inputs, mask=None, training=None, initial_state=None):\n",
    "    self.cell.reset_dropout_mask()\n",
    "    self.cell.reset_recurrent_dropout_mask()\n",
    "    return super(ECNN, self).call(\n",
    "        inputs, mask=mask, training=training, initial_state=initial_state)\n",
    "\n",
    "  @property\n",
    "  def units(self):\n",
    "    return self.cell.units\n",
    "\n",
    "  @property\n",
    "  def activation(self):\n",
    "    return self.cell.activation\n",
    "\n",
    "  @property\n",
    "  def use_bias(self):\n",
    "    return self.cell.use_bias\n",
    "\n",
    "  @property\n",
    "  def kernel_initializer(self):\n",
    "    return self.cell.kernel_initializer\n",
    "\n",
    "  @property\n",
    "  def recurrent_initializer(self):\n",
    "    return self.cell.recurrent_initializer\n",
    "\n",
    "  @property\n",
    "  def bias_initializer(self):\n",
    "    return self.cell.bias_initializer\n",
    "\n",
    "  @property\n",
    "  def kernel_regularizer(self):\n",
    "    return self.cell.kernel_regularizer\n",
    "\n",
    "  @property\n",
    "  def recurrent_regularizer(self):\n",
    "    return self.cell.recurrent_regularizer\n",
    "\n",
    "  @property\n",
    "  def bias_regularizer(self):\n",
    "    return self.cell.bias_regularizer\n",
    "\n",
    "  @property\n",
    "  def kernel_constraint(self):\n",
    "    return self.cell.kernel_constraint\n",
    "\n",
    "  @property\n",
    "  def recurrent_constraint(self):\n",
    "    return self.cell.recurrent_constraint\n",
    "\n",
    "  @property\n",
    "  def bias_constraint(self):\n",
    "    return self.cell.bias_constraint\n",
    "\n",
    "  @property\n",
    "  def dropout(self):\n",
    "    return self.cell.dropout\n",
    "\n",
    "  @property\n",
    "  def recurrent_dropout(self):\n",
    "    return self.cell.recurrent_dropout\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {\n",
    "        'units':\n",
    "            self.units,\n",
    "        'activation':\n",
    "            activations.serialize(self.activation),\n",
    "        'use_bias':\n",
    "            self.use_bias,\n",
    "        'kernel_initializer':\n",
    "            initializers.serialize(self.kernel_initializer),\n",
    "        'recurrent_initializer':\n",
    "            initializers.serialize(self.recurrent_initializer),\n",
    "        'bias_initializer':\n",
    "            initializers.serialize(self.bias_initializer),\n",
    "        'kernel_regularizer':\n",
    "            regularizers.serialize(self.kernel_regularizer),\n",
    "        'recurrent_regularizer':\n",
    "            regularizers.serialize(self.recurrent_regularizer),\n",
    "        'bias_regularizer':\n",
    "            regularizers.serialize(self.bias_regularizer),\n",
    "        'activity_regularizer':\n",
    "            regularizers.serialize(self.activity_regularizer),\n",
    "        'kernel_constraint':\n",
    "            constraints.serialize(self.kernel_constraint),\n",
    "        'recurrent_constraint':\n",
    "            constraints.serialize(self.recurrent_constraint),\n",
    "        'bias_constraint':\n",
    "            constraints.serialize(self.bias_constraint),\n",
    "        'dropout':\n",
    "            self.dropout,\n",
    "        'recurrent_dropout':\n",
    "            self.recurrent_dropout\n",
    "    }\n",
    "    base_config = super(ECNN, self).get_config()\n",
    "    del base_config['cell']\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "  @classmethod\n",
    "  def from_config(cls, config):\n",
    "    if 'implementation' in config:\n",
    "      config.pop('implementation')\n",
    "    return cls(**config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECNN implementation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from matplotlib import pylab as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input,Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_step  = 7\n",
    "batch_size = 512\n",
    "epochs     = 100\n",
    "neurons = 32\n",
    "Features   = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model_1(neurons,bs,Time_step,epochs,Features=6):\n",
    "    x1 = Input(batch_shape = (None,Time_step,Features))\n",
    "    x2 = ECNN(neurons)(x1)\n",
    "    x3 = Dense(1,activation='linear')(x2)\n",
    "    return Model(x1,x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_model_1(neurons,batch_size,Time_step,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScaleData(df,normalise=True):\n",
    "    if normalise:\n",
    "        scaler = MinMaxScaler()\n",
    "        for i in list(df.columns.values):\n",
    "            df[i] = scaler.fit_transform(df[i].values.reshape(-1, 1))\n",
    "    return df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"path-of-datafile\"\n",
    "dataP = pd.read_csv(data)\n",
    "data_new = dataP.drop([\"Date\",'Volume'],axis=1) # Removing the date colums\n",
    "data_new = data_new.dropna()\n",
    "Data, scaler = ScaleData(data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[\"Observed\"]= Data[\"Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = Data.values\n",
    "values.shape\n",
    "print(values[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values[:,-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SequenceToData(df, TimeStep):\n",
    "    \"\"\"Data formating into (nb samples, time step, features+1)\n",
    "    where +1 is the observed value\"\"\"\n",
    "    X,Y = [],[]\n",
    "    for idx in range(len(df)):\n",
    "        end_idx = idx+TimeStep\n",
    "        if end_idx > len(df)-1:\n",
    "            break\n",
    "        seq_X, seq_Y = df[idx:end_idx], df[:,3][end_idx]\n",
    "        X.append(seq_X)\n",
    "        Y.append(seq_Y)\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D is the data and L are the true values to be estimated\n",
    "D,L = SequenceToData(values,Time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the shape of the data. It has to be the length of the sequence-time_step\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def Batches(data,bc_sz,p):\n",
    "#    \n",
    "#    if len(data)< bc_sz:nb_bc_sz = 1\n",
    "#    else: nb_bc_sz = int(len(data)/bc_sz)\n",
    "#    Data = data[:nb_bc_sz*bc_sz]\n",
    "#    Trn_sz = int(nb_bc_sz*(1-p*2))*bc_sz\n",
    "#    Val_sz = int(nb_bc_sz*p)*bc_sz\n",
    "#    Tes_sz = int(len(Data)-(Trn_sz+Val_sz))\n",
    "#    \n",
    "#    return Data[:Trn_sz],Data[Trn_sz:Trn_sz+Val_sz],Data[Trn_sz+Val_sz:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train,x_valid,x_test = Batches(D,batch_size,0.1)\n",
    "# y_train,y_valid,y_test = Batches(L,batch_size,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = D[:5000],L[:5000]\n",
    "x_valid, y_valid = D[5000:6000], L[5000:6000]\n",
    "x_test, y_test   = D[6000:], L[6000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape,y_valid.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to a basel model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "Real = y_test[1:]       ## from the second up to the last\n",
    "Predicted = y_test[:-1] ## from the first up to the second but last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(Real, color=\"blue\", label=\"B: real \",linestyle='--', dashes=(5, 1),linewidth=2.5)\n",
    "plt.plot(Predicted, color=\"red\", label=\"B: predicted \");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE = \",np.sqrt(mean_squared_error(Real,Predicted)))\n",
    "print(\"r2 = \", r2_score(Real, Predicted))\n",
    "print(\"MAE\",mean_absolute_error(Real,Predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    NOTE: It is not possible to save the model when eager execution is enabled. Since by default is enable in tensorflow 2.0.0-beta, the workaround is to desable it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = [tensorflow.keras.callbacks.ModelCheckpoint(\"mymodel.h5\",monitor='val_loss',save_best_only=True,save_weights_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test), verbose=1, shuffle=False,\n",
    "                   callbacks=callback)\n",
    "model.load_weights('mymodel.h5')\n",
    "print(\"validation loss = \", model.evaluate(x_test,y_test,verbose=1,batch_size = batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test1 = model.predict(x_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=np.reshape(y_test, (y_test.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5));\n",
    "plt.plot(y_test, color=\"black\", label=\"Actual Value\")\n",
    "plt.plot(pred_test1, color=\"green\", label=\"Predicted Value\")\n",
    "plt.title('Predicted vs Actual on testig set')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Normalized Closing Price')\n",
    "plt.legend(loc='best');\n",
    "plt.legend(fontsize=20) # using a size in points\n",
    "plt.legend(fontsize=\"x-large\") # using a named size\n",
    "#plt.grid()\n",
    "plt.savefig(\"fi_NA_RW.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = scaler.inverse_transform(pred_test1)\n",
    "actual =scaler.inverse_transform(y_test)\n",
    "\n",
    "actual[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40, 10));\n",
    "#plt.figure(figsize=(15, 5));\n",
    "plt.plot(actual, color=\"black\", label=\"Real Price\",linewidth=2.5)\n",
    "plt.plot(pred_test, color=\"green\", label=\"ECNN without Exponential Smoothing\")\n",
    "#plt.title('future stock prices')\n",
    "#plt.xlabel('Days')\n",
    "plt.xlabel('time[days]',fontsize=\"large\")\n",
    "plt.ylabel('Closing price', fontsize=\"large\")\n",
    "plt.legend(loc='best');\n",
    "plt.legend(fontsize=20) # using a size in points\n",
    "plt.legend(fontsize=\"x-large\") # using a named size\n",
    "#plt.grid()\n",
    "#plt.grid(color='k', linestyle='--', linewidth=0.1)\n",
    "plt.savefig(\"ECNN without Exponential Smoothing.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.sqrt(mean_squared_error(actual, pred_test))\n",
    "print('Test RMSE: %.4f' % (score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(actual, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(actual, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "MAPE = mean_absolute_percentage_error(y_test, pred_test1)\n",
    "\n",
    "print(MAPE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
